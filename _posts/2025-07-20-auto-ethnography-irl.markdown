---
layout: post
title: "Auto Ethnography VLM"
date: 2025-07-20 00:00:00 +0000
image: /images/auto-vlm.png
categories: Cornell
course: Interaction Research Lab 
author: Audrey Tjokro
subtitle: Automated video ethnography tool using Google Gemini VLM for social interaction analysis
project-page: https://github.com/IRL-CT/auto-ethnography-vlm

---

Built an automated annotation pipeline using Google’s Gemini Vision-Language Model (VLM) to analyze human-robot interactions in urban deployment footage. This tool enables scalable video ethnography by generating structured JSON annotations of social behaviors—such as approaching, photographing, or helping—directly from raw video frames, reducing researcher annotation time from hours to minutes.

**Key Contributions:** Integrated Gemini VLM with video processing, frame sampling, and structured prompt engineering to detect HRI events with natural language descriptions and confidence scores. Validated outputs against human annotations across multiple NYC deployment sites.

**Technical Skills:** Vision-language models, multimodal prompt engineering, automated annotation, JSON data structuring, Python scripting, video frame extraction, Google Colab, Box API